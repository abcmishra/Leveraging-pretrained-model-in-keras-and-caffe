{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfHId531-K4V"
   },
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Transfer Learning and Fine-tuning </font>\n",
    "In this chapter, we will learn how to fine-tune a pre-trained model for a different task than it was originally trained for.\n",
    "\n",
    "When we train a network from scratch, we encounter the following two limitations :\n",
    "\n",
    "- Huge data required - Since the network has millions of parameters, to get an optimal set of parameters, we need to have a lot of data.\n",
    "- Huge computing power required - Even if we have a lot of data, training generally requires multiple iterations and it takes a toll on the computing resources.\n",
    "\n",
    "The pre-trained models are trained on very large scale image classification problems. The convolutional layers act as feature extractor and the fully connected layers act as Classifiers.\n",
    "\n",
    "Since these models are very large and have seen a huge number of images, they tend to learn very good, discriminative features. We can either use the convolutional layers merely as a feature extractor and change the last layer according to our problem or we can tweak the already trained convolutional layers to suit our problem at hand. The former approach is known as **Transfer Learning** and the latter as **Fine-tuning**.\n",
    "\n",
    "The task of fine-tuning a network is to tweak the parameters of an already trained network so that it adapts to the new task at hand. The initial layers of a network learn very general features and as we go higher up the network, the layers tend to learn patterns more specific to the task it is being trained on. Thus, for fine-tuning, we want to keep the initial layers intact ( or freeze them ) and retrain the later layers for our task.\n",
    "\n",
    "Thus, fine-tuning avoids both the limitations discussed above.\n",
    "\n",
    "The amount of data required for training is not much because of two reasons. \n",
    "- First, we are not training the entire network. Second, the part that is being trained is not trained from scratch.\n",
    "- Since the parameters that need to be updated is less, the amount of time needed will also be less.\n",
    "\n",
    "As a rule of thumb, when we have a small training set and our problem is similar to the task for which the pre-trained models were trained, we can use transfer learning. If we have enough data, we can try and tweak the convolutional layers so that they learn more robust features relevant to our problem. You can get a detailed overview of Fine-tuning and transfer learning [here](http://cs231n.github.io/transfer-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JB8MoYqlLfGe",
    "outputId": "8eb0ad4a-fdaa-4342-8f5d-5b873950e9d1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "import os,shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzIHAV2INW7m"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Loading kaggle dataset</font>\n",
    "We will be using the [kaggle dataset](https://www.kaggle.com/sriramr/apples-bananas-oranges) on fruits. The interesting thing about this dataset is that it contains 6 classes - 3 of which belong to normal fruits and 3 to rotten fruits!\n",
    "\n",
    "To add the dataset, go to the top right corner of the window and click on `Add Data`. Click on search by url and enter the url- https://www.kaggle.com/sriramr/apples-bananas-oranges. You should be able to see the dataset. Click on the Add button. This will add the dataset to the input folder - `/kaggle/input`. You can check using the `ls` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /kaggle/input/apples-bananas-oranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_rY_x8AMzUE"
   },
   "outputs": [],
   "source": [
    "# This folder contians all the class folders\n",
    "main_path = '/kaggle/input/apples-bananas-oranges/original_data_set/original_data_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vkZ-FKuDM2Jx",
    "outputId": "7b775866-44eb-4118-ac66-3909fc56f1d2"
   },
   "outputs": [],
   "source": [
    "# all class names\n",
    "classnames = os.listdir(main_path)\n",
    "print(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "nyVpV0xwNA7k",
    "outputId": "fb7f57d4-4447-42fb-f0e4-88a41f36bc04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See how many images you have per class, this helps you set the required percentage of validation data.\n",
    "for each_class in classnames:\n",
    "  print(\"Class: {}, has {} samples\".format( each_class,len(os.listdir(os.path.join(main_path,each_class )))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2v6UvQnNHbo"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Arrange your Data</font>\n",
    "Here we are going to place the data neatly in train and validation folders inside a main folder called 'fruits' (change the name if you want), you can set the percentage of validation set. We have set it to 15%. So 15% of images from each class will go to validation folder and the rest to train folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "4oDZVqpONBtA",
    "outputId": "1677dea8-99c5-434f-a65f-6ce2a38d33ca"
   },
   "outputs": [],
   "source": [
    "!rm -r fruits\n",
    "\n",
    "base_dir= 'fruits'\n",
    "\n",
    "total_train_images = 0\n",
    "total_val_images = 0\n",
    "\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'validation') \n",
    "os.mkdir(validation_dir)\n",
    "\n",
    "# Set the percent of validation data you want.\n",
    "validation_percent = 15\n",
    "\n",
    "for each_class in classnames:\n",
    "\n",
    "  source_directory = os.path.join(main_path,each_class )\n",
    "  destination_train_directory = os.path.join(train_dir,each_class)\n",
    "  destination_validation_directory = os.path.join(validation_dir,each_class)\n",
    "\n",
    "\n",
    "  total_image_count = len(os.listdir(source_directory))\n",
    "\n",
    "  valid_image_count = int(np.floor(total_image_count * (validation_percent/100)))\n",
    "\n",
    "  train_images_count = int( total_image_count - valid_image_count )\n",
    "\n",
    "  total_train_images +=  train_images_count\n",
    "  total_val_images += valid_image_count\n",
    "\n",
    "  os.mkdir(destination_train_directory)\n",
    "  os.mkdir(destination_validation_directory)\n",
    "\n",
    "  # copying the data to class's train folder\n",
    "  file_names = os.listdir(source_directory)[:train_images_count]\n",
    "\n",
    "  for fname in file_names:\n",
    "      src = os.path.join(source_directory, fname)\n",
    "      dst = os.path.join(destination_train_directory, fname)\n",
    "      shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "  # Copying the data to class's validation folder\n",
    "  file_names = os.listdir(source_directory)[train_images_count:]\n",
    "\n",
    "  for fname in file_names:\n",
    "      src = os.path.join(source_directory, fname)\n",
    "      dst = os.path.join(destination_validation_directory, fname)\n",
    "      shutil.copyfile(src, dst)\n",
    "  \n",
    "  print('total training {} images: {}'.format(each_class, len(os.listdir(destination_train_directory))))\n",
    "  print('total validation {} images: {}'.format(each_class, len(os.listdir(destination_validation_directory))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhQLJGt2-K4w"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Freeze the required layers</font>\n",
    "In Keras, each layer has a parameter called “trainable”. For freezing the weights of a particular layer, we should set this parameter to `False`, indicating that this layer should not be trained. That’s it! We go over each layer and select which layers we want to train.\n",
    "\n",
    "```\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcUHACyzLfHp"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Experiment 1: Freezing all layers - Same as Transfer Learning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "jva8MvWuLfHq",
    "outputId": "73642da7-41c9-4c93-ece3-f9f107a9b43c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Freeze all the layers\n",
    "for layer in vgg_conv.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(classnames), activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BjJ0uZ3LfHt"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Setup the data generators</font>\n",
    "\n",
    "We have already separated the data into train and validation and kept it in the “train” and “validation” folders. We can use ImageDataGenerator available in Keras to read images in batches directly from these folders and optionally perform data augmentation. We will use two different data generators for train and validation folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-izOmqvbDI7H",
    "outputId": "a08ac39e-e37c-41b8-c11e-acfb031df331"
   },
   "outputs": [],
   "source": [
    "# No Data augmentation \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 20\n",
    "val_batchsize = 20\n",
    "\n",
    "# Data Generator for Training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Data Generator for Validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m__N-xPIDLCl"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Train the model</font>\n",
    "Till now, we have created the model and set up the data for training. So, we should proceed with the training and check out the performance. We will have to specify the optimizer and the learning rate and start training using the model.fit() function. After the training is over, we will save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "4sWzFSVQDCkN",
    "outputId": "6b7f6b6b-22ca-4329-f70b-d24bd6561d3a"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=20,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnwP5BnME0G5"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Save the model</font>\n",
    "Keras models are saved in h5 format. You can just use the save method along with the name of the file you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VROrTe9jOqhF"
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model.save('all_freezed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtBVLTHdO5Oi"
   },
   "outputs": [],
   "source": [
    "# If you want to download your Model run this, you can also download it from the files tab on the left\n",
    "from google.colab import files\n",
    "files.download('all_freezed.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dutF19eCDZe6"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Check Performance</font>\n",
    "\n",
    "Let us see the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "iXKs7OdoOnUf",
    "outputId": "c87d38af-c1e7-4666-8af5-11154c828f83"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaF619WQLfHy"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Show the errors</font>\n",
    "Also, let us visually see the errors that we got.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8lBv849VLfHz",
    "outputId": "954697cb-7634-401a-813d-62e484c75df3"
   },
   "outputs": [],
   "source": [
    "# Create a generator for prediction\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    "\n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    "\n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    "\n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    "\n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    "\n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
    "\n",
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(predictions[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "    \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        predictions[errors[i]][pred_class])\n",
    "    \n",
    "    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUN0VMQXGkQq"
   },
   "source": [
    "We can see that the test loss is not converging even thought the training loss keeps decreasing. Let us try to train some more layers (Fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-gSnfe9LfH2"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Experiment 2 : Train Last 4 layers without data augmentation</font>\n",
    "\n",
    "In this experiment, we will keep the initial layers fixed and only retrain the last 4 layers. Since the initial layers learn more general features, it is a good practice to freeze the initial layers while fine-tuning the latter layers of the network for the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "colab_type": "code",
    "id": "J-PQPdbYLfH3",
    "outputId": "80518cfd-b4d1-4af1-fca5-94438d77b460"
   },
   "outputs": [],
   "source": [
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Freeze all layers except the last 4\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(classnames), activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71AWktPcLfH5"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Train the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "colab_type": "code",
    "id": "aF6raPLzLfH6",
    "outputId": "0b972685-f36d-4233-e896-c0cfe2da880d"
   },
   "outputs": [],
   "source": [
    "# No Data augmentation \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 50\n",
    "val_batchsize = 20\n",
    "\n",
    "# Data Generator for Training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Data Generator for Validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=20,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xv1hUtRFR0u4"
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model.save('last4_layers.h5')\n",
    "\n",
    "# you can also download the model to your pc from the files tab on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "xlf__PR7R1Ay",
    "outputId": "74ebfd8b-1b82-4e63-9fc8-8b92642d656e"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pbonc-sLfH-"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Show the errors</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fs5iIX0QLfH_",
    "outputId": "ca79851d-d87c-4073-c6a7-2b017653de00"
   },
   "outputs": [],
   "source": [
    "# Create a generator for prediction\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    "\n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    "\n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    "\n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    "\n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    "\n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
    "\n",
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(predictions[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "    \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        predictions[errors[i]][pred_class])\n",
    "    \n",
    "    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PH2XF5EqLfIB"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Experiment 3 : Train last 8 layers with data augmentation</font>\n",
    "\n",
    "We do this experiment to check if we can get even better results than the previous experiment. We train more number of layers and also use data augmentation while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "colab_type": "code",
    "id": "VNeQc1BXLfIC",
    "outputId": "574d6f8b-1f9d-4e3b-9b76-8b402cceb601"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Freeze all the layers\n",
    "for layer in vgg_conv.layers[:-8]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    "\n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(len(classnames), activation='softmax'))\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kkg4cATSLfIF"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Train the model</font>\n",
    "Here we will be using the imageDataGenerator for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "colab_type": "code",
    "id": "L1-oMd0nLfIG",
    "outputId": "189f6c43-03c5-4f1c-eb81-d28fb97f314c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 50\n",
    "val_batchsize = 20\n",
    "\n",
    "# Data Generator for Training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Data Generator for Validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the Model\n",
    "# NOTE that we have multiplied the steps_per_epoch by 2. This is because we are using data augmentation.\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=50,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRal0iIVUzzW"
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model.save('da_last4_layers.h5')\n",
    "\n",
    "# You can also download it from the files tab on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "wLsdzZA1Uz5i",
    "outputId": "121ac68e-cfc0-4f5f-8c5a-36cd87200c01"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAT21JABLfIJ"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Show the errors</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_mlqAyZULfIK",
    "outputId": "98228dbc-6824-47f4-c3e8-4d932871b68e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a generator for prediction\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    "\n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    "\n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    "\n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    "\n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    "\n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
    "\n",
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(predictions[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "    \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        predictions[errors[i]][pred_class])\n",
    "    \n",
    "    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ukYyuUiHPhT"
   },
   "source": [
    "In experiment 2, there were signs of overfitting, which are reduced to an extent by using data augmentation. You should try and run for more number of epochs to see the effect of data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rV9NjgyEHQW9"
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Exercise</font>\n",
    "\n",
    "1. Try to use a different dataset and see if the same code and network works for the new dataset.\n",
    "1. Try unfreezing more layers and see how the accuracy improves.\n",
    "1. Try with some different model instead of VGG"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "keras-finetune-vgg.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
